{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to train and compare several common Supervised Machine Learning models on the [1984 Congressional Voting Records Dataset](https://archive.ics.uci.edu/ml/datasets/congressional+voting+records \"Congressional Voting Records Data Set\"). \n",
    "\n",
    "The Machine Learning models used and compared here are several classifiers: K-Nearest Neighbors (KNN), Support-Vector Machine Classifier (SVC), Logistic Regression(LogReg), and the Random Forest Classifier. I learned to use these models from [Datacamp](http://www.datacamp.com)'s course on Supervised Learning, including training and evaluation of model performance. \n",
    "\n",
    "One common way to evaluate model performance is using the [Receiver Operating Characteristic Curve](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) (or ROC). We'll go into more detail later, but this gives us quick overview of what model parameters can give optimal performance in terms of true positives and false positives.\n",
    "<img src=\"https://illesial.github.io/images/ROC.png\" alt=\"ROC Curve\" width=\"360\" align=\"center\"/>\n",
    "<!--excerpt-->\n",
    "\n",
    "Each one of the models named above is meant to classify an observation. In this case, the observations are the voting records of members of the House of Representatives in 1984, and the desired classification is Party Affiliation - Republican or Democrat. \n",
    "\n",
    "Each model will be trained on a fraction of the dataset: a fraction of the voting records will be provided, along with the party affiliation of that voter. The models will then attempt to predict what the party affiliation is of the remaining data: based on the voting record of each congressional member, what party do they belong to?\n",
    "\n",
    "We'll begin by importing packages, starting with pandas as pd and numpy as np. We'll also our import some models and tools from scikit-learn (`sklearn`), including `train_test_split`, `GridSearchCV`, and `classification_report`, among others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### MODELS TO USE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# PARAMETER TUNING / CROSS VALIDATION TOOLS\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "### MODEL EVALUATION METRICS TO USE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load our data into a pandas dataframe df using `pd.read_csv()`. I've already cleaned the data ahead of time, and it's available in `house_reps_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('house_reps_data\\\\house_reps_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll inspect the data using `df.head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>democrat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2    3    4    5    6    7    8    9   10   11   12  \\\n",
       "0  republican  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0   \n",
       "1  republican  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "2    democrat  0.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "3    democrat  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "4    democrat  1.0  1.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "\n",
       "    13   14   15   16  \n",
       "0  1.0  1.0  0.0  1.0  \n",
       "1  1.0  1.0  0.0  0.0  \n",
       "2  1.0  1.0  0.0  0.0  \n",
       "3  1.0  0.0  0.0  1.0  \n",
       "4  1.0  1.0  1.0  1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each row corresponds to a member of Congress. The first column is party, and the rest of the columns record their vote: 1 for yes, 0 for no or abstain. \n",
    "\n",
    "Our goal is to predict, using several classification models, the party affiliation of a member of congress based on their voting record. To do so, we'll first need to split up our features `X` (the predictor variables) from our target variables `y`. We'll also need to convert our target variables, the party affiliation, to numerical data: 0 for republican and 1 for democrat. These steps are taken below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.apply(lambda x: {'republican': 0, 'democrat': 1}[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a model, it's useful to split your data into training sets and testings sets. This helps us make sure our model behaves better for predicting unseen data. Therefore we use `train_test_split()` below, and use only 60% of our data to train, and reserve 40% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models:\n",
    "### 1) KNN\n",
    "We proceed by training our first classification model on our voting data: K-Nearest Neighbors, or KNN. The way KNN works is that it groups points into classes based on their K-\"closest\" neighbors, where closest means Euclidean distance by default. \n",
    "\n",
    "How should we pick K? That is, how do we choose the number of neighbors to group together? Instead of picking values of K one at a time, we'll have python choose the best value of K, using `GridSearchCV`. This is called \"hyper-\"parameter tuning. Additionally, `GridSearchCV` will cross-validate; that is, it will split the data into training and validation sets multiple times, and test parameter values each time. In this way, `GridSearchCV` will better protect against overfitting while also choosing optimal parameters. Here, we'll have `GridSearchCV` determine the best value of K by trying each value from 1 to 49, and then we'll report the best model parameter and best model accuracy. Take note that the model parameter is actually denoted by the variable `n_neighbors` instead of K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN Parameter: {'n_neighbors': 7}\n",
      "Tuned KNN Accuracy: 0.9157088122605364\n"
     ]
    }
   ],
   "source": [
    "# Create the hyperparameter grid\n",
    "n_space = np.arange(1, 50)\n",
    "param_grid = {'n_neighbors': n_space}\n",
    "\n",
    "# Instantiate the logistic regression classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv = 5)\n",
    "\n",
    "# Fit it to the training data\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned KNN Parameter: {}\".format(knn_cv.best_params_))\n",
    "print(\"Tuned KNN Accuracy: {}\".format(knn_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN, Model evaluation:\n",
    "What we see is the optimal number of neighbors is 7, with an accuracy of approxmately 0.92. \n",
    "\n",
    "Below, we make model predictions using the test data, and we grade our model using several popular model scores: precision, recall, and the f1-score. These scores take values between zero and one. If precision is close to one, it means that the model rarely made a \"false positive\", while if recall is close to one, it means that the model correctly identified nearly all positives. The f1-score combines both precision and recall; if the f1-score is close to one, the model had a high rate of correct classification. Support here is just the number of occurences of each class. Recall that we previously set 0 to Republican, and 1 to Democrat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 55   4]\n",
      " [  7 108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.93      0.91        59\n",
      "          1       0.96      0.94      0.95       115\n",
      "\n",
      "avg / total       0.94      0.94      0.94       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_knn=knn_cv.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model evaluation tool we'll use is the ROC curve, which stands for the Receiver Operating Characteristic curve. The ROC curve is a graph of the true positive rate vs the false positive rate as a classification probability threshold used in the model varies. A desirable ROC curve has high true positive rates at low false positive rates; visually, this corresponds to a high area under this curve. Hence, we have another heuristic model score called AUC, or area under the curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTx4gJJp7WwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8863690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = knn_cv.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('KNN ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score for this model we calculate below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9766396462785557\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score is about 0.98, which is very good!\n",
    "\n",
    "### 2) Random Forest Classifier:\n",
    "Next, we'll train a Random Forest classifier on our data. A Random Forest classifier is a type of Decision Tree classifier. Decision tree classifiers work, and do so very accurately, by drawing many decision boundaries in order to classify the data. In fact, Decision trees are so flexible that they typically overfit. Although a Decision Tree may obtain a high accuracy score on its training data, there is no reason to expect this high accuracy will translate to new data. This overfitting can be combatted by training many Decision Trees on different parts of the training data, and aggregating the results. This is what is called a Random Forest, and the procedure that generates the random forest is frequently referred to as \"bagging\", or bootstrap aggregating. \n",
    "\n",
    "We'll implement the random forest below along with cross-validation and some parameter tuning. Here the model parameters are the number of estimators and the maximum number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': array([10, 11, 11, 12, 13, 14, 14, 15, 16, 17, 18, 19, 21, 22, 23, 25, 26,\n",
       "       28, 30, 32]), 'max_features': array([2, 3, 4, 5, 6, 7, 8])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = len(X_train.columns)\n",
    "n_est_space=np.array([round_to_int(x) for x in np.logspace(1, 1.5, num = 20)])\n",
    "max_feat_space=np.arange(round_to_int(np.sqrt(num_cols) / 2), round_to_int(np.sqrt(num_cols) * 2) + 1)\n",
    "\n",
    "param_grid_rf = {'n_estimators': n_est_space, 'max_features': max_feat_space}\n",
    "\n",
    "rand_forest = RandomForestClassifier()\n",
    "\n",
    "rand_forest_cv = GridSearchCV(rand_forest, param_grid_rf, cv = 5)\n",
    "rand_forest_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest Parameters: {'max_features': 3, 'n_estimators': 14}\n",
      "Tuned Random Forest Accuracy: 0.9578544061302682\n"
     ]
    }
   ],
   "source": [
    "print(\"Tuned Random Forest Parameters: {}\".format(rand_forest_cv.best_params_))\n",
    "print(\"Tuned Random Forest Accuracy: {}\".format(rand_forest_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest, Model evaluation:\n",
    "\n",
    "It looks like the best model parameters are 14 estimators and a maximum number of features of 3. The accuracy is about 0.96, which is better than KNN. \n",
    "\n",
    "We again use our model to predict on the test data, and report precision, recall, and f1-scores. This time, however, we'll leave out the ROC curve and AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57   2]\n",
      " [  4 111]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.97      0.95        59\n",
      "          1       0.98      0.97      0.97       115\n",
      "\n",
      "avg / total       0.97      0.97      0.97       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = rand_forest_cv.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest again performs better than KNN; its f1-score is 0.97. \n",
    "### 3) SVC Classifier\n",
    "The next model we'll train is the Support Vector Machine Classifier, or SVC for short. SVC attempts to classify the data by identifying a separating hyperplane; that is, it aims to find a linear boundary which separates the data into classes. This model has one regularization parameter, C, which allows the model to be more flexible (by breaking the decision boundary into pieces) at higher values. A value of C that is too high will lead to overfitting. Hence, we again use cross-validation and parameter tuning to identify the best value of this parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned SVC Parameter: {'C': 3.726530612244898}\n",
      "Tuned SVC Accuracy: 0.9540229885057471\n"
     ]
    }
   ],
   "source": [
    "C_space = np.linspace(.2, 5, num=50)\n",
    "param_grid_svc = {'C': C_space}\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc_cv = GridSearchCV(svc, param_grid_svc, cv = 5)\n",
    "svc_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned SVC Parameter: {}\".format(svc_cv.best_params_))\n",
    "print(\"Tuned SVC Accuracy: {}\".format(svc_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 58   1]\n",
      " [  3 112]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97        59\n",
      "          1       0.99      0.97      0.98       115\n",
      "\n",
      "avg / total       0.98      0.98      0.98       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_svc = svc_cv.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Classifier, Model Evaluation:\n",
    "Here we can see that SVC has high accuracy (0.95) and a high f1-score as well, 0.98. \n",
    "\n",
    "### 4) Logistic Regression Classifier\n",
    "Our final classifier is the Logistic Regression, which uses a sigmoidal activation function to classify the data. We again use cross validation with parameter tuning. Here our parameter, C, controls how steep the activation function can be, and again functions to give the model flexibility while protecting against overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 0.5918367346938775}\n",
      "Tuned Logistic Regression Accuracy: 0.9501915708812261\n"
     ]
    }
   ],
   "source": [
    "C_space_logreg = np.linspace(.2, 5, num=50)\n",
    "param_grid_logreg = {'C': C_space_logreg}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid_logreg, cv = 5)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57   2]\n",
      " [  2 113]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97        59\n",
      "          1       0.98      0.98      0.98       115\n",
      "\n",
      "avg / total       0.98      0.98      0.98       174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_logreg = logreg_cv.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression, Model evaluation:\n",
    "We see that the Logistic Regression has both a high accuracy (0.95) and a high f1-score (0.98). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we compared several (Supervised) Machine Learning classifiers: KNN, Random Forest, the Support Vector Machine classifier (SVC), and Logistic Regression. We trained each of them on the Congressional Voting Records dataset, with the goal being to predict the party affiliation of a member of Congress based on their voting record. We used cross-validation and parameter tuning, and evaluated each model using several popular model evaluation scores such as AUC and the f1-score, going beyond simple prediction accuracy. While each performed reasonably well, the Random Forest, SVC, and Logistic Regression had stronger accuracy and f1-scores during testing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
